{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:06:58.187971Z",
     "start_time": "2024-12-25T18:06:55.622567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# USE GPU 4\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) with a custom classification head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", num_classes=10, hidden_size=768, dropout_prob=0.3):\n",
    "        super(CustomViT, self).__init__()\n",
    "        self.base_model = ViTModel.from_pretrained(model_name, output_hidden_states=True)  # Pretrained ViT\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(hidden_size, hidden_size)  # Pre-classification head\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the ViT model and custom classification head.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains:\n",
    "                - logits (torch.Tensor): Output logits for classification.\n",
    "                - hidden_states (List[torch.Tensor]): Intermediate layer outputs.\n",
    "        \"\"\"\n",
    "        outputs = self.base_model(pixel_values=x, output_hidden_states=True)\n",
    "        embeddings = outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "        pre_logits = self.pre_classifier(embeddings)\n",
    "        pre_logits = torch.relu(pre_logits)\n",
    "        pre_logits = self.dropout(pre_logits)\n",
    "        logits = self.classifier(pre_logits)\n",
    "\n",
    "        return {\"logits\": logits, \"hidden_states\": outputs.hidden_states}\n",
    "\n"
   ],
   "id": "da2ff2512ca319dc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:06:58.205168Z",
     "start_time": "2024-12-25T18:06:58.202106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def ce_loss(model_outputs, labels,):\n",
    "\n",
    "    logits = model_outputs[\"logits\"]\n",
    "\n",
    "    # Compute Cross-Entropy Loss\n",
    "    ce_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    total_loss = ce_loss\n",
    "    return total_loss"
   ],
   "id": "a0a6a5ccbae774e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:06:58.387449Z",
     "start_time": "2024-12-25T18:06:58.383571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device, alpha, temperature):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "            # Predictions\n",
    "        logits = outputs[\"logits\"]\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute the combined loss\n",
    "        loss= ce_loss(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    metrics = compute_metrics(all_predictions, all_labels)\n",
    "    return total_loss / len(data_loader),metrics\n",
    "\n"
   ],
   "id": "15a3ab597ea5302f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:06:58.453612Z",
     "start_time": "2024-12-25T18:06:58.448295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def evaluate(model, data_loader, device, alpha, temperature):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move data to device\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the combined loss\n",
    "            loss = ce_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            # Predictions\n",
    "            logits = outputs[\"logits\"]\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_predictions, all_labels)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss,metrics"
   ],
   "id": "a520580e6ee3dd94",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:06:58.515944Z",
     "start_time": "2024-12-25T18:06:58.510916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ],
   "id": "93c846485bdc2978",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:49:28.434671Z",
     "start_time": "2024-12-25T18:06:58.556571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 512\n",
    "    learning_rate = 3e-5\n",
    "    num_epochs = 50  # Increased to allow patience mechanism to take effect\n",
    "    patience = 5  # Early stopping patience\n",
    "    alpha = 0.01 # Weight for SNNL (negative for regularization)\n",
    "    temperature = 0.1\n",
    "    device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to ViT input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=\"./data/CIFAR10/\", train=True, transform=transform, download=True)\n",
    "    val_dataset = datasets.CIFAR10(root=\"./data/CIFAR10/\", train=False, transform=transform, download=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers=8)\n",
    "\n",
    "    # Model setup\n",
    "    model = CustomViT(model_name=\"google/vit-base-patch16-224\", num_classes=10)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.base_model.parameters(), 'lr': 1e-5},  # Pre-trained layers\n",
    "        {'params': model.pre_classifier.parameters(), 'lr': 1e-4},  # Custom head\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss,train_metrics = train_one_epoch(model, train_loader, optimizer, device, alpha, temperature)\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, device, alpha, temperature)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\\n\")\n",
    "        print(\"Train Metrics:\\n\")\n",
    "        print(train_metrics)\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss:.4f}\\n\")\n",
    "        print(\"Validation Metrics:\\n\")\n",
    "        print(val_metrics)\n",
    "\n",
    "        # Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            # torch.save(model.state_dict(), \"/home/mdabed/Work/HealthLink/ViT/CIFAR100/best_vit_model.pt\") \n",
    "            torch.save(model.state_dict(), \"./checkpoints/best_vit_model.pt\")  # Save the best model\n",
    "            print(\"Best model saved.\")\n",
    "        \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience Counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ab4ef17aa3b09249",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:38<00:00,  3.46s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2632\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.92422, 'precision': 0.9245380926076147, 'recall': 0.92422, 'f1': 0.9242744002073242}\n",
      "Validation Loss: 0.0616\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9818, 'precision': 0.9818231495124277, 'recall': 0.9818, 'f1': 0.9818013494754272}\n",
      "Best model saved.\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:38<00:00,  3.46s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0382\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.98822, 'precision': 0.9882206198157771, 'recall': 0.98822, 'f1': 0.9882200718412921}\n",
      "Validation Loss: 0.0568\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9842, 'precision': 0.9843002380440148, 'recall': 0.9842, 'f1': 0.9842205753993947}\n",
      "Best model saved.\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:37<00:00,  3.45s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0194\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.99416, 'precision': 0.9941595610554753, 'recall': 0.99416, 'f1': 0.9941596632364026}\n",
      "Validation Loss: 0.0569\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9842, 'precision': 0.9842528076547982, 'recall': 0.9842, 'f1': 0.9842110353800719}\n",
      "Patience Counter: 1/5\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:37<00:00,  3.44s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0089\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9978, 'precision': 0.9977998220766938, 'recall': 0.9978, 'f1': 0.9977998054809406}\n",
      "Validation Loss: 0.0624\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.985, 'precision': 0.9851248309801595, 'recall': 0.985, 'f1': 0.9850134529585374}\n",
      "Patience Counter: 2/5\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:39<00:00,  3.46s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0051\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9987, 'precision': 0.9987004425266118, 'recall': 0.9987, 'f1': 0.9987000756380793}\n",
      "Validation Loss: 0.0655\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9848, 'precision': 0.9848942872730159, 'recall': 0.9848, 'f1': 0.9848120336429478}\n",
      "Patience Counter: 3/5\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:39<00:00,  3.46s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0029\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.99932, 'precision': 0.9993201158264805, 'recall': 0.99932, 'f1': 0.9993200259596065}\n",
      "Validation Loss: 0.0667\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9853, 'precision': 0.9853598427803819, 'recall': 0.9853, 'f1': 0.9853049108443752}\n",
      "Patience Counter: 4/5\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 98/98 [05:39<00:00,  3.46s/it]\n",
      "Evaluating: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0016\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.99978, 'precision': 0.999780027994401, 'recall': 0.99978, 'f1': 0.9997800039979999}\n",
      "Validation Loss: 0.0699\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.986, 'precision': 0.9860669310611389, 'recall': 0.986, 'f1': 0.9860077363014108}\n",
      "Patience Counter: 5/5\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is part to test the code",
   "id": "e8e2ac7ea1b10a78"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
