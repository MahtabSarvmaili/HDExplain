{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T21:55:53.361007Z",
     "start_time": "2024-12-25T21:55:50.845362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# USE GPU 4\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) with a custom classification head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", num_classes=10, hidden_size=768, dropout_prob=0.3):\n",
    "        super(CustomViT, self).__init__()\n",
    "        self.base_model = ViTModel.from_pretrained(model_name, output_hidden_states=True)  # Pretrained ViT\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(hidden_size, hidden_size)  # Pre-classification head\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        outputs = self.base_model(pixel_values=x, output_hidden_states=True)\n",
    "        embeddings = outputs.hidden_states[-1][:, 0, :]  # [CLS] token embedding\n",
    "        pre_logits = self.pre_classifier(embeddings)\n",
    "        pre_logits = torch.relu(pre_logits)\n",
    "        pre_logits = self.dropout(pre_logits)\n",
    "        logits = self.classifier(pre_logits)\n",
    "\n",
    "        return {\"logits\": logits, \"hidden_states\": outputs.hidden_states}\n",
    "\n"
   ],
   "id": "da2ff2512ca319dc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T21:55:53.377411Z",
     "start_time": "2024-12-25T21:55:53.374766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ce_loss(model_outputs, labels,):\n",
    "\n",
    "    logits = model_outputs[\"logits\"]\n",
    "\n",
    "    # Compute Cross-Entropy Loss\n",
    "    ce_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    total_loss = ce_loss\n",
    "    return total_loss"
   ],
   "id": "d66957f4cf0871ea",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T21:55:53.559865Z",
     "start_time": "2024-12-25T21:55:53.556391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device, alpha, temperature):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "            # Predictions\n",
    "        logits = outputs[\"logits\"]\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute the combined loss\n",
    "        loss= ce_loss(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    metrics = compute_metrics(all_predictions, all_labels)\n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n"
   ],
   "id": "15a3ab597ea5302f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T21:55:53.640015Z",
     "start_time": "2024-12-25T21:55:53.636710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def evaluate(model, data_loader, device, alpha, temperature):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move data to device\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the combined loss\n",
    "            loss = ce_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            # Predictions\n",
    "            logits = outputs[\"logits\"]\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_predictions, all_labels)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    return avg_loss,metrics\n",
    "\n"
   ],
   "id": "a520580e6ee3dd94",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T21:55:53.693971Z",
     "start_time": "2024-12-25T21:55:53.690884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ],
   "id": "93c846485bdc2978",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T23:58:19.485616Z",
     "start_time": "2024-12-25T21:55:53.734093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 512\n",
    "    learning_rate = 3e-5\n",
    "    num_epochs = 50  # Increased to allow patience mechanism to take effect\n",
    "    patience = 5  # Early stopping patience\n",
    "    alpha = 0.1 # Weight for SNNL (negative for regularization)\n",
    "    temperature = 0.1\n",
    "    device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Custom PyTorch Dataset class for SVHN\n",
    "    class SVHNDataset(Dataset):\n",
    "        def __init__(self, dataset, transform=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                dataset (Dataset): Hugging Face dataset object.\n",
    "                transform (callable, optional): A function/transform to apply to the images.\n",
    "            \"\"\"\n",
    "            self.dataset = dataset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Get the image and label\n",
    "            data = self.dataset[idx]\n",
    "            image, label = data[\"image\"], data[\"label\"]\n",
    "\n",
    "            # Apply transformations if provided\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "    # Data preparation\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),  # Resize to ViT input size\n",
    "        ToTensor(),  # Convert image to PyTorch Tensor\n",
    "        Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Load SVHN dataset using the Hugging Face datasets library\n",
    "    svhn_dataset = load_dataset('svhn', 'cropped_digits')\n",
    "    \n",
    "    # Wrap the training and test datasets with the custom class\n",
    "    train_dataset = SVHNDataset(svhn_dataset[\"train\"], transform=transform)\n",
    "    val_dataset = SVHNDataset(svhn_dataset[\"test\"], transform=transform)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # DataLoader setup\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    # Model setup\n",
    "    model = CustomViT(model_name=\"google/vit-base-patch16-224\", num_classes=10)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.base_model.parameters(), 'lr': 1e-5},  # Pre-trained layers\n",
    "        {'params': model.pre_classifier.parameters(), 'lr': 1e-4},  # Custom head\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    \n",
    "            # Define class names manually for SVHN\n",
    "    class_names = [str(i) for i in range(10)]\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, device, alpha, temperature)\n",
    "        val_loss,val_metrics = evaluate(model, val_loader, device, alpha, temperature)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\\n\")\n",
    "\n",
    "        # print(f\"Train Metrics: {train_metrics:.4f}\\n\\n\")\n",
    "        print(\"Train Metrics:\\n\")\n",
    "        print(train_metrics)\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss:.4f}\\n\")\n",
    "\n",
    "        print(\"Validation Metrics:\\n\")\n",
    "        print(val_metrics)\n",
    "\n",
    "        # Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            # torch.save(model.state_dict(), \"/home/mdabed/Work/HealthLink/ViT/CIFAR100/best_vit_model.pt\") \n",
    "            torch.save(model.state_dict(), \"./best_vit_model.pt\")  # Save the best model\n",
    "            print(\"Best model saved.\")\n",
    "            \n",
    "            print(\"Training complete for epoch \" + str(epoch))\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience Counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ab4ef17aa3b09249",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:19<00:00,  3.47s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4580\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.5133024830391635, 'precision': 0.504151736480484, 'recall': 0.5133024830391635, 'f1': 0.506271141529653}\n",
      "Validation Loss: 0.4944\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.8386985248924401, 'precision': 0.8413543595731304, 'recall': 0.8386985248924401, 'f1': 0.8389010827918356}\n",
      "Best model saved.\n",
      "Training complete for epoch 0\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4048\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.8701694036064813, 'precision': 0.8697572324780383, 'recall': 0.8701694036064813, 'f1': 0.8698498637559067}\n",
      "Validation Loss: 0.2643\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9204440688383528, 'precision': 0.9209556716109282, 'recall': 0.9204440688383528, 'f1': 0.9203856649274629}\n",
      "Best model saved.\n",
      "Training complete for epoch 1\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:19<00:00,  3.47s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2639\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9205127155084156, 'precision': 0.92036661520121, 'recall': 0.9205127155084156, 'f1': 0.9204018129030876}\n",
      "Validation Loss: 0.2139\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9378841425937308, 'precision': 0.9388787013301353, 'recall': 0.9378841425937308, 'f1': 0.9380072876961845}\n",
      "Best model saved.\n",
      "Training complete for epoch 2\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2097\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9385178208225835, 'precision': 0.9384448313052786, 'recall': 0.9385178208225835, 'f1': 0.9384598190487219}\n",
      "Validation Loss: 0.1939\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9432237246465888, 'precision': 0.94427286112175, 'recall': 0.9432237246465888, 'f1': 0.9433623359720066}\n",
      "Best model saved.\n",
      "Training complete for epoch 3\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:18<00:00,  3.46s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1754\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.949820494969764, 'precision': 0.9497689340580543, 'recall': 0.949820494969764, 'f1': 0.9497828899433367}\n",
      "Validation Loss: 0.1835\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.946681007990166, 'precision': 0.9478150418044692, 'recall': 0.946681007990166, 'f1': 0.946762290239128}\n",
      "Best model saved.\n",
      "Training complete for epoch 4\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1468\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9584067051612816, 'precision': 0.9583721389895654, 'recall': 0.9584067051612816, 'f1': 0.9583801995805615}\n",
      "Validation Loss: 0.1805\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9488322065150584, 'precision': 0.9500242740774072, 'recall': 0.9488322065150584, 'f1': 0.9489349195781908}\n",
      "Best model saved.\n",
      "Training complete for epoch 5\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:21<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1233\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9666380004641195, 'precision': 0.9666191927847873, 'recall': 0.9666380004641195, 'f1': 0.966622547266776}\n",
      "Validation Loss: 0.1764\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9517516902274125, 'precision': 0.952628201963602, 'recall': 0.9517516902274125, 'f1': 0.9518414723518794}\n",
      "Best model saved.\n",
      "Training complete for epoch 6\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:21<00:00,  3.49s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1008\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9731356730414841, 'precision': 0.9731246587721414, 'recall': 0.9731356730414841, 'f1': 0.9731256560237133}\n",
      "Validation Loss: 0.1761\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9521358328211432, 'precision': 0.9530261002459469, 'recall': 0.9521358328211432, 'f1': 0.9522255141980037}\n",
      "Best model saved.\n",
      "Training complete for epoch 7\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0815\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9796060444735657, 'precision': 0.9796000066124304, 'recall': 0.9796060444735657, 'f1': 0.9795997678629219}\n",
      "Validation Loss: 0.1805\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9522894898586355, 'precision': 0.9531125965962836, 'recall': 0.9522894898586355, 'f1': 0.9523679796885798}\n",
      "Patience Counter: 1/5\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0637\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.984083432299985, 'precision': 0.9840848906009041, 'recall': 0.984083432299985, 'f1': 0.9840818702087709}\n",
      "Validation Loss: 0.1866\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9528657037492317, 'precision': 0.9536008067180672, 'recall': 0.9528657037492317, 'f1': 0.9529317210838614}\n",
      "Patience Counter: 2/5\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.47s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0495\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9872367145801766, 'precision': 0.9872346525817138, 'recall': 0.9872367145801766, 'f1': 0.9872349176178806}\n",
      "Validation Loss: 0.1980\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9514443761524278, 'precision': 0.9523788013830301, 'recall': 0.9514443761524278, 'f1': 0.9515983271947018}\n",
      "Patience Counter: 3/5\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0387\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9904172980056514, 'precision': 0.9904181479812193, 'recall': 0.9904172980056514, 'f1': 0.9904172673674045}\n",
      "Validation Loss: 0.2210\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9495620774431469, 'precision': 0.9508022263546175, 'recall': 0.9495620774431469, 'f1': 0.949658582643273}\n",
      "Patience Counter: 4/5\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 144/144 [08:20<00:00,  3.48s/it]\n",
      "Evaluating: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0314\n",
      "\n",
      "Train Metrics:\n",
      "\n",
      "{'accuracy': 0.9916595001160299, 'precision': 0.9916591077180384, 'recall': 0.9916595001160299, 'f1': 0.991659139926665}\n",
      "Validation Loss: 0.2291\n",
      "\n",
      "Validation Metrics:\n",
      "\n",
      "{'accuracy': 0.9491011063306699, 'precision': 0.9502473064198784, 'recall': 0.9491011063306699, 'f1': 0.9492282261005789}\n",
      "Patience Counter: 5/5\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is for the function implementations",
   "id": "279ebd58c919ba85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from explainers import BaseExplainer\n",
    "from transformers import ViTModel\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self, W):\n",
    "        super(Sigmoid, self).__init__()\n",
    "        self.W = Variable(W, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate output and L2 regularizer\n",
    "        H = torch.matmul(x, self.W.transpose(0, 1))\n",
    "        Phi = torch.sigmoid(H)\n",
    "        W1 = torch.squeeze(self.W)\n",
    "        L2 = torch.sum(torch.mul(W1, W1))\n",
    "        return Phi, L2\n",
    "\n",
    "class RepresenterPointSelection(BaseExplainer):\n",
    "    def __init__(self, classifier, n_classes, gpu=False, **kwargs):\n",
    "        super(RepresenterPointSelection, self).__init__(classifier, n_classes, gpu)\n",
    "        if self.gpu:\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "            self.model = Sigmoid(classifier.classifier.weight.data.cpu().detach().cuda())\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "            self.model = Sigmoid(classifier.classifier.weight.data.detach())\n",
    "\n",
    "    def data_influence(self, train_loader, cache=True, lmbd=0.003, epoch=3000, **kwargs):\n",
    "        Xrepresentation = []\n",
    "        pred = []\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            Xtensor, _ = data\n",
    "            if self.gpu:\n",
    "                Xtensor = Xtensor.cuda()\n",
    "            Xrepresentation.append(self.classifier.base_model(pixel_values=Xtensor)[\"hidden_states\"][-1][:, 0, :].data.detach())\n",
    "            pred.append(self.classifier(Xtensor)[\"logits\"].data.detach())\n",
    "\n",
    "        Xrepresentation = torch.vstack(Xrepresentation)\n",
    "        pred = torch.vstack(pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            Xrepresentation = Xrepresentation.cuda()\n",
    "            pred = pred.cuda()\n",
    "\n",
    "        alpha = self.retrain(Xrepresentation, pred, self.model, lmbd, epoch)\n",
    "        self.influence = (alpha, self.to_np(Xrepresentation))\n",
    "\n",
    "    def _data_influence(self, X):\n",
    "        Xtensor = X\n",
    "        if self.gpu:\n",
    "            Xtensor = Xtensor.cuda()\n",
    "        Xrepresentation = self.classifier.base_model(pixel_values=Xtensor)[\"hidden_states\"][-1][:, 0, :].data.detach()\n",
    "        pred = self.classifier(Xtensor)[\"logits\"].data.detach()\n",
    "        return self.to_np(F.one_hot(torch.argmax(pred, dim=1), num_classes=self.n_classes)), self.to_np(Xrepresentation)\n",
    "\n",
    "    def pred_explanation(self, train_loader, X_test, topK=5):\n",
    "        X_test_tensor = torch.from_numpy(np.array(X_test, dtype=np.float32))\n",
    "        test_pred_label, test_representation = self._data_influence(X_test_tensor)\n",
    "        alpha, train_representation = self.influence\n",
    "        alpha_j = np.matmul(alpha, test_pred_label.T)\n",
    "\n",
    "        representation_similarity = np.matmul(train_representation, test_representation.T)\n",
    "\n",
    "        scores = (representation_similarity * alpha_j).T\n",
    "        return np.argpartition(scores, -topK, axis=1)[:, -topK:], scores\n",
    "\n",
    "    def data_debugging(self, train_loader):\n",
    "        y = []\n",
    "        for _, ytensor in train_loader:\n",
    "            y.append(ytensor)\n",
    "\n",
    "        y = self.to_np(torch.cat(y))\n",
    "        alpha, _ = self.influence\n",
    "        alpha_j = alpha[range(alpha.shape[0]), y]\n",
    "        return alpha_j, np.argsort(alpha_j)\n",
    "\n",
    "    def retrain(self, x, y, model, lmbd, epoch):\n",
    "        # Fine tune the last layer\n",
    "        min_loss = 10000.0\n",
    "        optimizer = optim.SGD([model.W], lr=1.0)\n",
    "        N = len(y)\n",
    "        for epoch in range(epoch):\n",
    "            phi_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            (Phi, L2) = model(x)\n",
    "            loss = L2 * lmbd + F.binary_cross_entropy(Phi.float(), y.float())\n",
    "            phi_loss += self.to_np(F.binary_cross_entropy(Phi.float(), y.float()))\n",
    "            loss.backward()\n",
    "            temp_W = model.W.data\n",
    "            grad_loss_W = self.to_np(torch.mean(torch.abs(model.W.grad)))\n",
    "            # Save the W with lowest loss\n",
    "            if grad_loss_W < min_loss:\n",
    "                if epoch == 0:\n",
    "                    init_grad = grad_loss_W\n",
    "                min_loss = grad_loss_W\n",
    "                best_W = temp_W\n",
    "                if min_loss < init_grad / 200:\n",
    "                    print('Stopping criteria reached in epoch :{}'.format(epoch))\n",
    "                    break\n",
    "            self.backtracking_line_search(model, model.W.grad, x, y, loss, lambda_l2=lmbd)\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch:{:4d}\\tloss:{}\\tphi_loss:{}\\tgrad:{}'.format(epoch, self.to_np(loss), phi_loss, grad_loss_W))\n",
    "        # Calculate w based on the representer theorem's decomposition\n",
    "        temp = torch.matmul(x, Variable(best_W).transpose(0, 1))\n",
    "        sigmoid_value = torch.sigmoid(temp)\n",
    "        # Derivative of sigmoid+BCE\n",
    "        weight_matrix = sigmoid_value - y\n",
    "        weight_matrix = torch.div(weight_matrix, (-2.0 * lmbd * N))\n",
    "        return self.to_np(weight_matrix)\n",
    "\n",
    "    # Implementation for backtracking line search\n",
    "    def backtracking_line_search(self, model, grad_w, x, y, val, lambda_l2=0.001):\n",
    "        t = 10.0\n",
    "        beta = 0.5\n",
    "        W_O = self.to_np(model.W)\n",
    "        grad_np_w = self.to_np(grad_w)\n",
    "        while True:\n",
    "            model.W = Variable(torch.from_numpy(W_O - t * grad_np_w).type(self.dtype), requires_grad=True)\n",
    "            val_n = 0.0\n",
    "            (Phi, L2) = model(x)\n",
    "            val_n = F.binary_cross_entropy(Phi.float(), y.float()) + L2 * lambda_l2\n",
    "            if t < 0.0000000001:\n",
    "                # Print \"t too small\"\n",
    "                break\n",
    "            if self.to_np(val_n - val + t * (torch.norm(grad_w) ** 2) / 2) >= 0:\n",
    "                t = beta * t\n",
    "            else:\n",
    "                break\n"
   ],
   "id": "81273acf3d6aa8c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "580b6e72971c3bf8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
